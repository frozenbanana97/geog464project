{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Jupyter notebook will be used to determine the user defines variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Retrieve Location of Desired Study Area\n",
    "Using either a shapefile, coordinate, or WKT the study area for where satalite imagery will be downloaded is to be determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import download\n",
    "from download import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find data using a geographic search\n",
    "To begin, we will need to craft a suitable WKT. ASF's [Vertex](https://search.asf.alaska.edu) can be helpful in this regard, as it allows you to draw on a map, or import a geospatial file such as a shapefile or geojson, after which a WKT string can be copied and used elsewhere. <br><br>\n",
    "This method is good for bulk downloads of all scenes matching the description, excercise with caution as it will download the max limit (currently set at 10) and can take awhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given location is: POLYGON((-61.0632 -20.9625,-59.9217 -20.9625,-59.9217 -20.425,-61.0632 -20.425,-61.0632 -20.9625))\n"
     ]
    }
   ],
   "source": [
    "#Select location using a WKT obtained from ASF's Vertex search site or copied from a geodatabase\n",
    "#example: POLYGON((-61.0632 -20.9625,-59.9217 -20.9625,-59.9217 -20.425,-61.0632 -20.425,-61.0632 -20.9625))\n",
    "wktlocation = download.location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date: 2020-01-01\n",
      "Does start date match format? : True\n"
     ]
    }
   ],
   "source": [
    "#set start date for search\n",
    "dateStart = download.startDate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Date: 2020-01-10\n",
      "Does end date match format? : True\n"
     ]
    }
   ],
   "source": [
    "#set end date for search\n",
    "dateEnd = download.endDate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding data using an existing scene name\n",
    "If you already have the name of the scene (i.e. 'S1A_EW_GRDM_1SDH_20150205T122016_20150205T122116_004488_005812_F6B0') you can use this part of the program. This is the recommended method as it allows the user to visually verify the scene before downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#create blank list OR run to reset current list\n",
    "scene_list = []\n",
    "print(scene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S1B_IW_GRDH_1SDV_20200607T223317_20200607T223346_021933_0299FF_1064']\n"
     ]
    }
   ],
   "source": [
    "#add scenes to the list to search using ASF's Vertex online app, run as many times as needed to append the list\n",
    "#to follow the example please input 'S1B_IW_GRDH_1SDV_20200607T223317_20200607T223346_021933_0299FF_1064'\n",
    "scene_list.append(input('input the scene here (i.e. \\'S1B_IW_GRDH_1SDV_20200607T223317_20200607T223346_021933_0299FF_1064'))\n",
    "print(scene_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing type of download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 results found\n"
     ]
    }
   ],
   "source": [
    "#choose type of product search\n",
    "resultChoice=input('Type either: centroid, geo, or scene depending on the search criteria used')\n",
    "\n",
    "if resultChoice == 'centroid':\n",
    "    results = download.centroid_loc(wktlocation, dateStart, dateEnd)\n",
    "elif resultChoice == 'geo':\n",
    "    results = download.geo_loc(wktlocation, dateStart, dateEnd)\n",
    "elif resultChoice == 'scene':\n",
    "    results = download.scene_loc(scene_list)\n",
    "else:\n",
    "    print('Error, please try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login to downlaod data\n",
    "\n",
    "You to have an [Earthdata Login](https://urs.earthdata.nasa.gov/) account to access the download function. The easiest way to check that your EDL account is in order is to simply go to [Vertex](https://search.asf.alaska.edu) and try to download a product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "#login to ASF Search with Earthdata account\n",
    "user_session = download.ASF_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data\n",
    "The scene search will download the specific file the user found in [Vertex](https://search.asf.alaska.edu), this may be prefered to geographic search.<br><br>\n",
    "The geographic search methods are good for bulk downloads of all scenes matching the description, excercise with caution as it will download the max limit (currently set at 10) and can take awhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the downlaod directory, or create it if it doesnt exist\n",
    "download.dir_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downlaods can take awhile, with files capable of being mulitple GB each, please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kyle\\OneDrive\\OneDriveDocuments\\Coding\\ice-cover-change\\venv-ice\\lib\\site-packages\\asf_search\\download\\download.py:61: UserWarning: File already exists, skipping download: ./downloads\\S1B_IW_GRDH_1SDV_20200607T223317_20200607T223346_021933_0299FF_1064.zip\n",
      "  warnings.warn(f'File already exists, skipping download: {os.path.join(path, filename)}')\n",
      "c:\\Users\\Kyle\\OneDrive\\OneDriveDocuments\\Coding\\ice-cover-change\\venv-ice\\lib\\site-packages\\asf_search\\download\\download.py:61: UserWarning: File already exists, skipping download: ./downloads\\S1B_IW_GRDH_1SDV_20200607T223317_20200607T223346_021933_0299FF_1064.iso.xml\n",
      "  warnings.warn(f'File already exists, skipping download: {os.path.join(path, filename)}')\n"
     ]
    }
   ],
   "source": [
    "download.download_products(results, user_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation\n",
    "I have provided geotiffs in the data folder already to save the trouble of downloading and georeferencing. If self downloaded use following cell to ensure georeferencing is done to the same projection as shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip downloaded products\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path('.')\n",
    "\n",
    "for f in p.glob('downloads/*.zip'):\n",
    "    with zipfile.ZipFile(f, 'r') as archive:\n",
    "        archive.extractall(path=f'./downloads/{f.stem}')\n",
    "        print(f'Done {f.stem}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raster Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip raster images with polygon for study site\n",
    "import fiona\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from rasterio import plot\n",
    "from rasterio.plot import show\n",
    "\n",
    "crs = rasterio.crs.CRS({\"init\": \"epsg:4326\"})\n",
    "\n",
    "with fiona.open(\"data\\location_polygon.shp\", \"r\") as shapefile:\n",
    "    shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "\n",
    "with rasterio.open('data\\s1b-iw-grd-vv-20200607t223317-20200607t223346-021933-0299ff-001_gr.tiff') as src:\n",
    "    #src.crs = crs #could not figure this one out... have to open the tif is QGIS and export it with the proper CRS\n",
    "    out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "    out_meta = src.meta\n",
    "\n",
    "out_meta.update({\"driver\": \"GTiff\",\n",
    "                 \"height\": out_image.shape[1],\n",
    "                 \"width\": out_image.shape[2],\n",
    "                 \"transform\": out_transform})\n",
    "\n",
    "with rasterio.open(\"data/clipped.tif\", \"w\", **out_meta) as dest:\n",
    "    dest.write(out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = rasterio.open('data/clipped.tif')\n",
    "data = ds.read()\n",
    "\n",
    "max = data.max()\n",
    "mean = data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reclassification\n",
    "import numpy as np\n",
    "\n",
    "lista = data.copy()\n",
    "\n",
    "lista[np.where((lista >= 0) & (lista <= mean/2))] = 1 #deforested\n",
    "lista[np.where((lista >= mean/2) & (lista <= mean))] = 2 #meh\n",
    "lista[np.where((lista >= mean) & (lista <= max))] = 3 #forest\n",
    "\n",
    "with rasterio.open('data/reclass.tif', 'w', \n",
    "driver=ds.driver,\n",
    "height=ds.height,\n",
    "width=ds.width,\n",
    "count=ds.count,\n",
    "transform=ds.transform,\n",
    "dtype=data.dtype) as dst:\n",
    "    dst.write(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "with rasterio.open('data/clipped.tif') as src:\n",
    "    data = src.read(\n",
    "        out_shape=(\n",
    "            src.count,\n",
    "            int(src.height * 1/2),\n",
    "            int(src.width * 1/2)\n",
    "        ),\n",
    "        resampling=Resampling.bilinear)\n",
    "\n",
    "# scale image transform\n",
    "    transform = src.transform * src.transform.scale(\n",
    "        (src.width / data.shape[-1]),\n",
    "        (src.height / data.shape[-2])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "\n",
    "src = rasterio.open(\"data/reclass.tif\")\n",
    "fig, ax = pyplot.subplots(1, figsize=(12, 12))\n",
    "show((src, 1), cmap='Greys_r', interpolation='none', ax=ax)\n",
    "matplotlib.axes._subplots.AxesSubplot\n",
    "show((src, 1), contour=True, ax=ax)\n",
    "matplotlib.axes._subplots.AxesSubplot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2020=rasterio.open('data/clipped20200607.tif')\n",
    "img2021=rasterio.open('data/clipped20210614.tif')\n",
    "img=rasterio.open('data/clipped.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now the files are ready for maipulation and analysis :D"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2da65eb68450e4ce2e658af4fd1bdf44adc91c5c2dd325c9f2d088ea218e5bb3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('venv-ice': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
